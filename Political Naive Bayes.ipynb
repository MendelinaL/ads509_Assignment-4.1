{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes on Political Text - Mendelina Lopez\n",
    "\n",
    "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details. You can download the required DB from the shared dropbox or from blackboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mendi\\Anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Feel free to include your text patterns functions\n",
    "#from text_functions_solutions import clean_tokenize, get_patterns\n",
    "\n",
    "# Some punctuation variations\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "\n",
    "# Stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "def remove_punctuation(text, punct_set=tw_punct) : \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def tokenize(text) : \n",
    "    return(word_tokenize(text))\n",
    "\n",
    "def prepare(text, pipeline) : \n",
    "    tokens = str(text)\n",
    "    \n",
    "    for transform in pipeline : \n",
    "        tokens = transform(tokens)\n",
    "        \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('conventions',)]\n"
     ]
    }
   ],
   "source": [
    "convention_db = sqlite3.connect(\"2020_Conventions.db\")\n",
    "convention_cur = convention_db.cursor()\n",
    "\n",
    "# I would like to see if there are other tables\n",
    "convention_cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(convention_cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "party\n",
      "night\n",
      "speaker\n",
      "speaker_count\n",
      "time\n",
      "text\n",
      "text_len\n",
      "file\n"
     ]
    }
   ],
   "source": [
    "# I would like to view the columns within the table\n",
    "data = convention_cur.execute('''SELECT * FROM conventions''')\n",
    "for column in data.description:\n",
    "    print(column[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" class work. First, pull in the text \n",
    "for each party and prepare it for use in Naive Bayes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline = [str.lower, remove_punctuation, tokenize]\n",
    "convention_data = []\n",
    "\n",
    "# fill this list up with items that are themselves lists. The \n",
    "# first element in the sublist should be the cleaned and tokenized\n",
    "# text in a single string. The second element should be the party. \n",
    "\n",
    "query_results = convention_cur.execute(\n",
    "                            '''\n",
    "                            SELECT text, party FROM conventions\n",
    "                            WHERE speaker != \"Unknown\"\n",
    "                            ''')\n",
    "\n",
    "for row in query_results :\n",
    "    # store the results in convention_data\n",
    "    text = \" \".join(prepare(row[0],pipeline=my_pipeline))\n",
    "    if text :\n",
    "        convention_data.append([\" \".join(prepare(row[0],pipeline=my_pipeline)),row[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random entries and see if they look right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['we ’ re not waiting for washington to act on climate change here in boise we know that clean energy doesn ’ t just mean a healthier planet it means goodpaying jobs imagine what we could do with a president that listens to science and leads with courage idaho casts 9 votes for bernie sanders and 16 for our next president joe biden',\n",
       "  'Democratic'],\n",
       " ['we have to get our sports back we can ’ t wait to see what ’ s next never give up never lose faith never ever quit just keep forging ahead give everything you ’ ve got victory is always within reach',\n",
       "  'Republican'],\n",
       " ['now some politicians try to pit us against each other but i believe that americans have more in common than what divides us and in november we have a chance to elect a president who believes that too i ’ ve known joe for more than 40 years i met him as a wideeyed law student and he ’ s been my friend and champion ever since the joe i know is exactly the leader our country needs right now he can bring people together to find common ground while standing up for what he believes is right after years of bitter partisanship he can unite our country and get things done for working families and everyone looking for a better future because it ’ s not about what side of the aisle we ’ re on it ’ s about whether or not we ’ re on the side of the people',\n",
       "  'Democratic'],\n",
       " ['instead of protecting us you tore our world apart my mom is a good person and she ’ s not a criminal now my mom is gone and she ’ s been taken from us for no reason at all every day that passes you deport more moms and dads and take them away from kids like me',\n",
       "  'Democratic'],\n",
       " ['in the house of representatives we ’ re closing loopholes to ensure local infrastructure projects use americanmade materials and local labor and support american manufacturing',\n",
       "  'Democratic'],\n",
       " ['his entire campaign was for all americans that was a turning point for me',\n",
       "  'Republican'],\n",
       " ['they had no fear but america didn ’ t stop there we looked into the sky and kept pressing onward we built a 6 million pound rocket and launched it thousands of miles into space we did it so that two brave patriots could stay until and salute our wondrous american flag planted on the face of the moon for america nothing is impossible over the next four years we will prove worthy of this magnificent legacy we will reach stunning new heights and we will show that the world for america there is a dream and it is not beyond your reach together we are unstoppable together we are unbeatable because together we are the proud citizens of the united states of america and on november 3rd we will make america safer we will make america stronger we will make america prouder and we will make america greater than ever before i am very very proud to be the nominee of the republican party i love you all god bless you and god bless america thank you very much other related transcripts andrew cuomo covid19 announcement transcript september 16 • 56 mins ago donald trump abc news town hall transcript with george stephanopoulos in philadelphia • 4 hours ago joe biden hispanic heritage month event speech transcript september 15 • 16 hours ago rnc night 4 rnc night 4 president trump secretary of housing and urban development ben carson and ivanka trump will speak on the final night of the republican national convention watch the speeches live posted by npr on thursday august 27 2020 stay updated get a weekly digest of the week ’ s most important transcripts in your inbox it ’ s the news without the news please enable javascript in your browser to complete this form email what kind of transcripts do you want to read submit transcription overview how it works faq mobile app captions overview how it works faq caption converter subtitles translation overview certified translation business translation faq request a quote about company press careers freelancers blog api get in touch 222 kearny st 8th floor san francisco ca 94108 contact us 8883690701 supportrevcom © revcom reviews terms privacy stay updated get a weekly digest of the week ’ s most important transcripts in your inbox it ’ s the news without the news please enable javascript in your browser to complete this form email what kind of transcripts do you want to read submit stay updated get a weekly digest of the week ’ s most important transcripts in your inbox it ’ s the news without the news please enable javascript in your browser to complete this form email what kind of transcripts do you want to read submit stay updated get a weekly digest of the week ’ s most important transcripts in your inbox it ’ s the news without the news please enable javascript in your browser to complete this form email what kind of transcripts do you want to read submit',\n",
       "  'Republican'],\n",
       " ['leftists try to turn them into villains they want to cancel them but i ’ m here to tell you these heroes can ’ t be canceled tennessee is full of them after all we ’ re the volunteer state my dad served in the army in world war ii when he came home he put on another uniform and for 30 years volunteered to help our underfunded sheriff ’ s department i ’ m reminded of him when ever i see compassion and selflessness and others when i see law enforcement officers put their lives on the line every single day to keep our community safe in spite of the hatred thrown at them when i see the heroes who volunteer to serve our country putting their lives on the line for our freedom many of these heroes called tennessee home and we could not be more proud of the brave men and women of the 101st airborne division at fort campbell the common thread between them is a deep seated desire to serve a cause larger than themselves they don ’ t believe their country owes them anything they believe they owe their country and their fellow man as hard as democrats try they can ’ t cancel our heroes they can ’ t contest their bravery and they can ’ t dismiss the powerful sense of service that lives deep in their souls they tried to defund them our military our police even ice to take away their tools to keep us safe joe biden kamala harris and their radical allies try to destroy these heroes because if there are no heroes to inspire us government can control us they close our churches but keep the liquor stores and abortion clinics open they say we can ’ t gather in community groups but encourage protests riots and looting in the streets',\n",
       "  'Republican'],\n",
       " ['i know very well', 'Republican'],\n",
       " ['i ’ m an example of a woman who has been given a second chance in life god bless you and god bless america good evening i ’ m alice marie johnson i was once told that the only way i would ever be reunited with my family would be as a corpse but by the grace of god and the compassion of president donald john trump i stand before you tonight and i assure you i ’ m not a ghost i am alive i am well and most importantly i am free in 1996 i began serving time in prison life plus 25 years i had never been in trouble i was a first time nonviolent offender what i did was wrong i made decisions that i regret',\n",
       "  'Republican']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(convention_data,k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good, we now need to make our function to turn these into features. In my solution, I wanted to keep the number of features reasonable, so I only used words that occur at least `word_cutoff` times. Here's the code to test that if you want it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 5, we have 2441 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "word_cutoff = 5\n",
    "\n",
    "tokens = [w for t, p in convention_data for w in t.split()]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_features(text,fw) :\n",
    "    \"\"\"Given some text, this returns a dictionary holding the\n",
    "       feature words.\n",
    "       \n",
    "       Args: \n",
    "            * text: a piece of text in a continuous string. Assumes\n",
    "            text has been cleaned and case folded.\n",
    "            * fw: the *feature words* that we're considering. A word \n",
    "            in `text` must be in fw in order to be returned. This \n",
    "            prevents us from considering very rarely occurring words.\n",
    "        \n",
    "       Returns: \n",
    "            A dictionary with the words in `text` that appear in `fw`. \n",
    "            Words are only counted once. \n",
    "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
    "            then this would return a dictionary of \n",
    "            {'quick' : True,\n",
    "             'fox' :    True}\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    cv = CountVectorizer()\n",
    "    cv.fit([text])\n",
    "    CountVectorizer(analyzer='word',binary=False,decode_error='strict',\n",
    "                   encoding='utf-8',input='content',\n",
    "                   lowercase=True,max_df=1.0,max_features=None,min_df=1,\n",
    "                   ngram_range=(1,1),preprocessor=None,stop_words=None,\n",
    "                   strip_accents=None,token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "                   tokenizer=None,vocabulary=None)\n",
    "    \n",
    "    ret_dict = dict(cv.get_feature_names([fw]))\n",
    "    \n",
    "    return([ret_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mendi\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_feature_names() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-be3e6a1914ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m assert(conv_features(\"donald is the president\",feature_words)==\n\u001b[0m\u001b[0;32m      3\u001b[0m        {'donald':True,'president':True})\n\u001b[0;32m      4\u001b[0m assert(conv_features(\"people are american in america\",feature_words)==\n\u001b[0;32m      5\u001b[0m                      {'america':True,'american':True,\"people\":True})\n",
      "\u001b[1;32m<ipython-input-16-21b34c068bb1>\u001b[0m in \u001b[0;36mconv_features\u001b[1;34m(text, fw)\u001b[0m\n\u001b[0;32m     29\u001b[0m                    tokenizer=None,vocabulary=None)\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mret_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mret_dict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFutureWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_feature_names() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "assert(len(feature_words)>0)\n",
    "assert(conv_features(\"donald is the president\",feature_words)==\n",
    "       {'donald':True,'president':True})\n",
    "assert(conv_features(\"people are american in america\",feature_words)==\n",
    "                     {'america':True,'american':True,\"people\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(conv_features(text,feature_words), party) for (text, party) in convention_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a little prose here about what you see in the classifier. Anything odd or interesting?\n",
    "\n",
    "### My Observations\n",
    "\n",
    "_Your observations to come._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
    "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
    "give you the query I used to pull out the tweets. Note that this DB has some big tables and \n",
    "is unindexed, so the query takes a minute or two to run on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(\"congressional_data.db\")\n",
    "cong_cur = cong_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('websites',), ('candidate_data',), ('tweets',)]\n"
     ]
    }
   ],
   "source": [
    "cong_cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cong_cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cong_cur.execute(\n",
    "        '''\n",
    "           SELECT DISTINCT \n",
    "                  cd.candidate, \n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd \n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle \n",
    "               AND cd.candidate == tw.candidate \n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic') \n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "        ''')\n",
    "\n",
    "results = list(results) # Just to store it, since the query is time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = []\n",
    "\n",
    "# Now fill up tweet_data with sublists like we did on the convention speeches.\n",
    "# Note that this may take a bit of time, since we have a lot of tweets.\n",
    "for rowt in results :\n",
    "    # store the results in convention_data\n",
    "    textt = \" \".join(prepare(rowt[2],pipeline=my_pipeline))\n",
    "    if textt :\n",
    "        tweet_data.append([rowt[0],rowt[1],\" \".join(prepare(rowt[2],pipeline=my_pipeline))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Jimmy Panetta',\n",
       "  'Democratic',\n",
       "  'bearlier today i spoke on the house floor abt protecting health care for women and praised ppmarmonte for their work on the central coast httpstcowqgtrzt7vv'],\n",
       " ['Marcy Kaptur',\n",
       "  'Democratic',\n",
       "  'bgo tribe # rallytogether httpstco0nxutfl9l5'],\n",
       " ['Debbie Wasserman Schultz',\n",
       "  'Democratic',\n",
       "  'bapparently trump thinks its just too easy for students overwhelmed by the crushing burden of debt to pay off student loans # trumpbudget httpstcockyqo5t0qh'],\n",
       " ['Dave Brat',\n",
       "  'Republican',\n",
       "  'bwexe2x80x99re grateful for our first responders our rescue personnel our firefighters our police and volunteers who have been working tirelessly to keep people safe provide muchneeded help while putting their own lives on the linennhttpstcoezpv0vmiz3'],\n",
       " ['Antonio Sabàto Jr',\n",
       "  'Republican',\n",
       "  'bletxe2x80x99s make it even greater # kag xf0x9fx87xbaxf0x9fx87xb8 httpstcoy9qozd5l2z'],\n",
       " ['Marcia Fudge',\n",
       "  'Democratic',\n",
       "  'bwe have about 1hr until the cavs tie up the series 22 im # allin216 repbarbaralee you scared # roadtovictory'],\n",
       " ['Scott Peters',\n",
       "  'Democratic',\n",
       "  'bcongrats to belliottsd on his new gig at sd city hall we are glad you will continue to servexe2x80xa6 httpstcofkvmw3cqdi'],\n",
       " ['Mariah Phillips',\n",
       "  'Democratic',\n",
       "  'bwe are really close we have over 3500 raised toward the match right now whoot thatxe2x80x99s 7000 for the nonmath majors in the room xf0x9fx98x82 help us get there httpstcotu34c472sd httpstcoqsdqkypsmc'],\n",
       " ['Jimmy Panetta',\n",
       "  'Democratic',\n",
       "  'btoday the comment period for potusxe2x80x99s plan to expand offshore drilling opened to the public you have 60 days until march 9 to share why you oppose the proposed program directly with the trump administration comments can be made by email or mail httpstcobaaymejxqn'],\n",
       " ['Lucille Roybal-Allard',\n",
       "  'Democratic',\n",
       "  'bcelebrated icseastlaxe2x80x99s 22 years of eastside commitment amp saluted community leaders at last nightxe2x80x99s awards dinner httpstco7v7gh8givb']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(20201014)\n",
    "\n",
    "tweet_data_sample = random.choices(tweet_data,k=10)\n",
    "tweet_data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-48be623fe3e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparty\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweet_data_sample\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mestimated_party\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_data_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Fill in the right-hand side above with code that estimates the actual party\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Here's our (cleaned) tweet: {tweet}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "for tweet, party in tweet_data_sample :\n",
    "    estimated_party = random.choices(tweet_data_sample[1],k=1)\n",
    "    # Fill in the right-hand side above with code that estimates the actual party\n",
    "    \n",
    "    print(f\"Here's our (cleaned) tweet: {tweet}\")\n",
    "    print(f\"Actual party is {party} and our classifer says {estimated_party}.\")\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at it some, let's score a bunch and see how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of counts by actual party and estimated party. \n",
    "# first key is actual, second is estimated\n",
    "parties = ['Republican','Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties :\n",
    "    for p1 in parties :\n",
    "        results[p][p1] = 0\n",
    "\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(tweet_data) :\n",
    "    tweet, party = tp    \n",
    "    # Now do the same thing as above, but we store the results rather\n",
    "    # than printing them. \n",
    "   \n",
    "    # get the estimated party\n",
    "    estimated_party = \"Gotta fill this in\"\n",
    "    \n",
    "    results[party][estimated_party] += 1\n",
    "    \n",
    "    if idx > num_to_score : \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "_Write a little about what you see in the results_ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
