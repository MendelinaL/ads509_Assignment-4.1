{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes on Political Text - Mendelina Lopez\n",
    "\n",
    "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details. You can download the required DB from the shared dropbox or from blackboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Feel free to include your text patterns functions\n",
    "#from text_functions_solutions import clean_tokenize, get_patterns\n",
    "\n",
    "# Some punctuation variations\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "\n",
    "# Stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "def remove_punctuation(text, punct_set=tw_punct) : \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def tokenize(text) : \n",
    "    return(word_tokenize(text))\n",
    "\n",
    "def prepare(text, pipeline) : \n",
    "    tokens = str(text)\n",
    "    \n",
    "    for transform in pipeline : \n",
    "        tokens = transform(tokens)\n",
    "        \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('conventions',)]\n"
     ]
    }
   ],
   "source": [
    "convention_db = sqlite3.connect(\"2020_Conventions.db\")\n",
    "convention_cur = convention_db.cursor()\n",
    "\n",
    "# I would like to see if there are other tables\n",
    "convention_cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(convention_cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "party\n",
      "night\n",
      "speaker\n",
      "speaker_count\n",
      "time\n",
      "text\n",
      "text_len\n",
      "file\n"
     ]
    }
   ],
   "source": [
    "# I would like to view the columns within the table\n",
    "data = convention_cur.execute('''SELECT * FROM conventions''')\n",
    "for column in data.description:\n",
    "    print(column[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" class work. First, pull in the text \n",
    "for each party and prepare it for use in Naive Bayes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline = [str.lower, remove_punctuation, tokenize]\n",
    "convention_data = []\n",
    "\n",
    "# fill this list up with items that are themselves lists. The \n",
    "# first element in the sublist should be the cleaned and tokenized\n",
    "# text in a single string. The second element should be the party. \n",
    "\n",
    "query_results = convention_cur.execute(\n",
    "                            '''\n",
    "                            SELECT text, party FROM conventions\n",
    "                            WHERE speaker != \"Unknown\"\n",
    "                            ''')\n",
    "\n",
    "for row in query_results :\n",
    "    # store the results in convention_data\n",
    "    text = \" \".join(prepare(row[0],pipeline=my_pipeline))\n",
    "    if text :\n",
    "        convention_data.append([\" \".join(prepare(row[0],pipeline=my_pipeline)),row[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random entries and see if they look right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i want to acknowledge the fact that since march our lives have changed drastically the invisible enemy covid19 swept across our beautiful country and impacted all of us my deepest sympathy goes out to everyone who has lost a loved one and my prayers are with those who are ill or suffering i know many people are anxious and some feel helpless i want you to know you ’ re not alone my husband ’ s administration will not stop fighting until there is an effective treatment or vaccine available to everyone donald will not rest until he has done all he can to take care of everyone impacted by this terrible pandemic i want to extend my gratitude to all of the healthcare professionals frontline workers and teachers who stepped up in these difficult times despite the risk to yourselves and your own families you put our country first and my husband and i are grateful',\n",
       "  'Republican'],\n",
       " ['he ’ s even proposed education freedom scholarships to return control to parents protect religious liberties and empower kids to escape dangerous lowperforming schools the republican platform supports educational freedom the democrat party does not democrats stand with deceptive teachers unions who pick on loving teachers and little kids president trump stands with america ’ s families great teachers and most importantly our children so america ’ s great teachers let ’ s stand with president trump in protection of the kids and country we love',\n",
       "  'Republican'],\n",
       " ['in the white house', 'Democratic'],\n",
       " ['we have conducted 40 million more tests than the next closest nation which is india we developed a wide array of effective treatments including a powerful antibody treatment known as convalescent plasma you saw that on sunday night when we announced it that will save thousands and thousands of lives thanks to advances we have pioneered the fatality rate and you look at it and you look at the numbers it has been reduced by 80 since april 80 the united states has among the lowest case fatality rates of any major country anywhere in the world the european union ’ s case fatality rate is nearly three times higher than ours but you don ’ t hear that they don ’ t write about that they don ’ t want to write about that they don ’ t want you to know those things',\n",
       "  'Republican'],\n",
       " ['ladies and gentlemen the president of the united states', 'Republican'],\n",
       " ['i ’ m pat lynch president of the police benevolent association speaking to you on behalf of 50000 active and retired new york city police officers we are the proud men and women who wear a shield on our chest and put ourselves in harm ’ s way to protect our city we are proud to endorse our president donald j trump for reelection because the stakes have never been higher like cops across the country we are staring down the barrel of a public safety disaster more than 1000 people have been shot in new york city so far this year almost 300 have been killed these are not just numbers these are real people',\n",
       "  'Republican'],\n",
       " ['they hoped to raise 15 billion one person who did not want to talk about it joe biden',\n",
       "  'Republican'],\n",
       " ['and speaking of president obama a man i was honored to serve alongside for eight years as vice president let me take this moment to say something we don ’ t say nearly enough thank you mr president you were a great president a president that our children could and did look up to no one ’ s going to say that about the current occupant of the white house what we know about this president is that if he ’ s given four more years he ’ ll be what he ’ s been for the last four years the president takes no responsibility refuses to lead blames others cozies up to dictators and fans the flames of hate and division he ’ ll wake up every day believing that job is all about him never about you is that the american you want for you your family your children i see a different america one that ’ s generous and strong selfless and humble it ’ s an america we could rebuild together',\n",
       "  'Democratic'],\n",
       " ['okay i think i ‘ ll give it to jamie what do you think', 'Republican'],\n",
       " ['good evening i ’ m tiffany trump since speaking at the republican convention four years ago so much has changed for the world for our country and for my family like so many students across the world i graduated from law school during the pandemic our generation is unified in facing the future in uncertain times and many of us are considering what kind of country we want to live in',\n",
       "  'Republican']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(convention_data,k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good, we now need to make our function to turn these into features. In my solution, I wanted to keep the number of features reasonable, so I only used words that occur at least `word_cutoff` times. Here's the code to test that if you want it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 5, we have 2441 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "word_cutoff = 5\n",
    "\n",
    "tokens = [w for t, p in convention_data for w in t.split()]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_features(text,fw) :\n",
    "    \"\"\"Given some text, this returns a dictionary holding the\n",
    "       feature words.\n",
    "       \n",
    "       Args: \n",
    "            * text: a piece of text in a continuous string. Assumes\n",
    "            text has been cleaned and case folded.\n",
    "            * fw: the *feature words* that we're considering. A word \n",
    "            in `text` must be in fw in order to be returned. This \n",
    "            prevents us from considering very rarely occurring words.\n",
    "        \n",
    "       Returns: \n",
    "            A dictionary with the words in `text` that appear in `fw`. \n",
    "            Words are only counted once. \n",
    "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
    "            then this would return a dictionary of \n",
    "            {'quick' : True,\n",
    "             'fox' :    True}\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    cv = CountVectorizer()\n",
    "    cv.fit(text)\n",
    "    CountVectorizer(analyzer='word',binary=False,decode_error='strict',\n",
    "                   encoding='utf-8',input='content',\n",
    "                   lowercase=True,max_df=1.0,maxfeatures=None,min_df=1,\n",
    "                   ngram_range=(1,1),preprocessor=None,stopwords=None,\n",
    "                   strip_accents=None,token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "                   tokenizer=None,vocabulary=None)\n",
    "    \n",
    "    ret_dict = dict(cv.get_feature_names([fw]))\n",
    "    \n",
    "    return(ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-be3e6a1914ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m assert(conv_features(\"donald is the president\",feature_words)==\n\u001b[0m\u001b[0;32m      3\u001b[0m        {'donald':True,'president':True})\n\u001b[0;32m      4\u001b[0m assert(conv_features(\"people are american in america\",feature_words)==\n\u001b[0;32m      5\u001b[0m                      {'america':True,'american':True,\"people\":True})\n",
      "\u001b[1;32m<ipython-input-26-718ec947acd9>\u001b[0m in \u001b[0;36mconv_features\u001b[1;34m(text, fw)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     CountVectorizer(analyzer='word',binary=False,decode_error='strict',\n\u001b[0;32m     25\u001b[0m                    \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1281\u001b[0m         \"\"\"\n\u001b[0;32m   1282\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1283\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1284\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m             raise ValueError(\n\u001b[1;32m-> 1310\u001b[1;33m                 \u001b[1;34m\"Iterable over raw text documents expected, string object received.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1311\u001b[0m             )\n\u001b[0;32m   1312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "assert(len(feature_words)>0)\n",
    "assert(conv_features(\"donald is the president\",feature_words)==\n",
    "       {'donald':True,'president':True})\n",
    "assert(conv_features(\"people are american in america\",feature_words)==\n",
    "                     {'america':True,'american':True,\"people\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(conv_features(text,feature_words), party) for (text, party) in convention_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a little prose here about what you see in the classifier. Anything odd or interesting?\n",
    "\n",
    "### My Observations\n",
    "\n",
    "_Your observations to come._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
    "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
    "give you the query I used to pull out the tweets. Note that this DB has some big tables and \n",
    "is unindexed, so the query takes a minute or two to run on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(\"congressional_data.db\")\n",
    "cong_cur = cong_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('websites',), ('candidate_data',), ('tweets',)]\n"
     ]
    }
   ],
   "source": [
    "cong_cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cong_cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cong_cur.execute(\n",
    "        '''\n",
    "           SELECT DISTINCT \n",
    "                  cd.candidate, \n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd \n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle \n",
    "               AND cd.candidate == tw.candidate \n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic') \n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "        ''')\n",
    "\n",
    "results = list(results) # Just to store it, since the query is time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = []\n",
    "\n",
    "# Now fill up tweet_data with sublists like we did on the convention speeches.\n",
    "# Note that this may take a bit of time, since we have a lot of tweets.\n",
    "for rowt in results :\n",
    "    # store the results in convention_data\n",
    "    textt = \" \".join(prepare(rowt[2],pipeline=my_pipeline))\n",
    "    if textt :\n",
    "        tweet_data.append([rowt[0],rowt[1],\" \".join(prepare(rowt[2],pipeline=my_pipeline))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Jimmy Panetta',\n",
       "  'Democratic',\n",
       "  'bearlier today i spoke on the house floor abt protecting health care for women and praised ppmarmonte for their work on the central coast httpstcowqgtrzt7vv'],\n",
       " ['Marcy Kaptur',\n",
       "  'Democratic',\n",
       "  'bgo tribe # rallytogether httpstco0nxutfl9l5'],\n",
       " ['Debbie Wasserman Schultz',\n",
       "  'Democratic',\n",
       "  'bapparently trump thinks its just too easy for students overwhelmed by the crushing burden of debt to pay off student loans # trumpbudget httpstcockyqo5t0qh'],\n",
       " ['Dave Brat',\n",
       "  'Republican',\n",
       "  'bwexe2x80x99re grateful for our first responders our rescue personnel our firefighters our police and volunteers who have been working tirelessly to keep people safe provide muchneeded help while putting their own lives on the linennhttpstcoezpv0vmiz3'],\n",
       " ['Antonio Sabàto Jr',\n",
       "  'Republican',\n",
       "  'bletxe2x80x99s make it even greater # kag xf0x9fx87xbaxf0x9fx87xb8 httpstcoy9qozd5l2z'],\n",
       " ['Marcia Fudge',\n",
       "  'Democratic',\n",
       "  'bwe have about 1hr until the cavs tie up the series 22 im # allin216 repbarbaralee you scared # roadtovictory'],\n",
       " ['Scott Peters',\n",
       "  'Democratic',\n",
       "  'bcongrats to belliottsd on his new gig at sd city hall we are glad you will continue to servexe2x80xa6 httpstcofkvmw3cqdi'],\n",
       " ['Mariah Phillips',\n",
       "  'Democratic',\n",
       "  'bwe are really close we have over 3500 raised toward the match right now whoot thatxe2x80x99s 7000 for the nonmath majors in the room xf0x9fx98x82 help us get there httpstcotu34c472sd httpstcoqsdqkypsmc'],\n",
       " ['Jimmy Panetta',\n",
       "  'Democratic',\n",
       "  'btoday the comment period for potusxe2x80x99s plan to expand offshore drilling opened to the public you have 60 days until march 9 to share why you oppose the proposed program directly with the trump administration comments can be made by email or mail httpstcobaaymejxqn'],\n",
       " ['Lucille Roybal-Allard',\n",
       "  'Democratic',\n",
       "  'bcelebrated icseastlaxe2x80x99s 22 years of eastside commitment amp saluted community leaders at last nightxe2x80x99s awards dinner httpstco7v7gh8givb']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(20201014)\n",
    "\n",
    "tweet_data_sample = random.choices(tweet_data,k=10)\n",
    "tweet_data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-e42988ee3fac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparty\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweet_data_sample\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mestimated_party\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Gotta fill this in'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Fill in the right-hand side above with code that estimates the actual party\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Here's our (cleaned) tweet: {tweet}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "for tweet, party in tweet_data_sample :\n",
    "    estimated_party = 'Gotta fill this in'\n",
    "    # Fill in the right-hand side above with code that estimates the actual party\n",
    "    \n",
    "    print(f\"Here's our (cleaned) tweet: {tweet}\")\n",
    "    print(f\"Actual party is {party} and our classifer says {estimated_party}.\")\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at it some, let's score a bunch and see how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of counts by actual party and estimated party. \n",
    "# first key is actual, second is estimated\n",
    "parties = ['Republican','Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties :\n",
    "    for p1 in parties :\n",
    "        results[p][p1] = 0\n",
    "\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(tweet_data) :\n",
    "    tweet, party = tp    \n",
    "    # Now do the same thing as above, but we store the results rather\n",
    "    # than printing them. \n",
    "   \n",
    "    # get the estimated party\n",
    "    estimated_party = \"Gotta fill this in\"\n",
    "    \n",
    "    results[party][estimated_party] += 1\n",
    "    \n",
    "    if idx > num_to_score : \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "_Write a little about what you see in the results_ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
